{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.997167138810198,
  "eval_steps": 500,
  "global_step": 2382,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.012590494176896443,
      "grad_norm": 1.1557903289794922,
      "learning_rate": 4.981108312342569e-05,
      "loss": 4.7093,
      "step": 10
    },
    {
      "epoch": 0.025180988353792885,
      "grad_norm": 1.6940299272537231,
      "learning_rate": 4.960117548278758e-05,
      "loss": 4.603,
      "step": 20
    },
    {
      "epoch": 0.03777148253068933,
      "grad_norm": 1.9590779542922974,
      "learning_rate": 4.9391267842149456e-05,
      "loss": 4.5352,
      "step": 30
    },
    {
      "epoch": 0.05036197670758577,
      "grad_norm": 2.3326094150543213,
      "learning_rate": 4.918136020151134e-05,
      "loss": 4.2469,
      "step": 40
    },
    {
      "epoch": 0.06295247088448222,
      "grad_norm": 2.33305025100708,
      "learning_rate": 4.897145256087321e-05,
      "loss": 4.0903,
      "step": 50
    },
    {
      "epoch": 0.07554296506137866,
      "grad_norm": 2.267080068588257,
      "learning_rate": 4.87615449202351e-05,
      "loss": 3.83,
      "step": 60
    },
    {
      "epoch": 0.0881334592382751,
      "grad_norm": 1.7803550958633423,
      "learning_rate": 4.8551637279596976e-05,
      "loss": 3.4521,
      "step": 70
    },
    {
      "epoch": 0.10072395341517154,
      "grad_norm": 1.4287621974945068,
      "learning_rate": 4.834172963895886e-05,
      "loss": 3.2574,
      "step": 80
    },
    {
      "epoch": 0.11331444759206799,
      "grad_norm": 1.4198907613754272,
      "learning_rate": 4.813182199832074e-05,
      "loss": 3.1266,
      "step": 90
    },
    {
      "epoch": 0.12590494176896444,
      "grad_norm": 1.611326813697815,
      "learning_rate": 4.7921914357682624e-05,
      "loss": 3.0901,
      "step": 100
    },
    {
      "epoch": 0.13849543594586086,
      "grad_norm": 1.3001002073287964,
      "learning_rate": 4.77120067170445e-05,
      "loss": 3.0038,
      "step": 110
    },
    {
      "epoch": 0.1510859301227573,
      "grad_norm": 1.4049115180969238,
      "learning_rate": 4.750209907640638e-05,
      "loss": 2.9566,
      "step": 120
    },
    {
      "epoch": 0.16367642429965376,
      "grad_norm": 1.3684141635894775,
      "learning_rate": 4.729219143576826e-05,
      "loss": 2.983,
      "step": 130
    },
    {
      "epoch": 0.1762669184765502,
      "grad_norm": 1.3256220817565918,
      "learning_rate": 4.7082283795130145e-05,
      "loss": 2.9194,
      "step": 140
    },
    {
      "epoch": 0.18885741265344666,
      "grad_norm": 1.4691916704177856,
      "learning_rate": 4.687237615449202e-05,
      "loss": 2.8993,
      "step": 150
    },
    {
      "epoch": 0.20144790683034308,
      "grad_norm": 1.5643856525421143,
      "learning_rate": 4.666246851385391e-05,
      "loss": 2.8669,
      "step": 160
    },
    {
      "epoch": 0.21403840100723953,
      "grad_norm": 1.3838163614273071,
      "learning_rate": 4.6452560873215786e-05,
      "loss": 2.8915,
      "step": 170
    },
    {
      "epoch": 0.22662889518413598,
      "grad_norm": 1.4400185346603394,
      "learning_rate": 4.624265323257767e-05,
      "loss": 2.9224,
      "step": 180
    },
    {
      "epoch": 0.23921938936103243,
      "grad_norm": 1.4320005178451538,
      "learning_rate": 4.603274559193955e-05,
      "loss": 2.852,
      "step": 190
    },
    {
      "epoch": 0.2518098835379289,
      "grad_norm": 1.4978665113449097,
      "learning_rate": 4.582283795130143e-05,
      "loss": 2.8609,
      "step": 200
    },
    {
      "epoch": 0.26440037771482533,
      "grad_norm": 1.5714550018310547,
      "learning_rate": 4.561293031066331e-05,
      "loss": 2.8946,
      "step": 210
    },
    {
      "epoch": 0.2769908718917217,
      "grad_norm": 1.3844926357269287,
      "learning_rate": 4.540302267002519e-05,
      "loss": 2.8338,
      "step": 220
    },
    {
      "epoch": 0.2895813660686182,
      "grad_norm": 1.5922244787216187,
      "learning_rate": 4.519311502938707e-05,
      "loss": 2.85,
      "step": 230
    },
    {
      "epoch": 0.3021718602455146,
      "grad_norm": 1.4395021200180054,
      "learning_rate": 4.4983207388748955e-05,
      "loss": 2.792,
      "step": 240
    },
    {
      "epoch": 0.3147623544224111,
      "grad_norm": 1.5016740560531616,
      "learning_rate": 4.477329974811083e-05,
      "loss": 2.8592,
      "step": 250
    },
    {
      "epoch": 0.3273528485993075,
      "grad_norm": 1.481205940246582,
      "learning_rate": 4.456339210747271e-05,
      "loss": 2.8345,
      "step": 260
    },
    {
      "epoch": 0.33994334277620397,
      "grad_norm": 1.4741297960281372,
      "learning_rate": 4.43534844668346e-05,
      "loss": 2.8139,
      "step": 270
    },
    {
      "epoch": 0.3525338369531004,
      "grad_norm": 1.621498703956604,
      "learning_rate": 4.4143576826196475e-05,
      "loss": 2.8917,
      "step": 280
    },
    {
      "epoch": 0.36512433112999687,
      "grad_norm": 1.3844609260559082,
      "learning_rate": 4.393366918555836e-05,
      "loss": 2.8621,
      "step": 290
    },
    {
      "epoch": 0.3777148253068933,
      "grad_norm": 1.5972840785980225,
      "learning_rate": 4.372376154492024e-05,
      "loss": 2.8629,
      "step": 300
    },
    {
      "epoch": 0.3903053194837897,
      "grad_norm": 1.6076381206512451,
      "learning_rate": 4.351385390428212e-05,
      "loss": 2.8492,
      "step": 310
    },
    {
      "epoch": 0.40289581366068616,
      "grad_norm": 1.4987024068832397,
      "learning_rate": 4.3303946263643995e-05,
      "loss": 2.8131,
      "step": 320
    },
    {
      "epoch": 0.4154863078375826,
      "grad_norm": 1.6247228384017944,
      "learning_rate": 4.309403862300588e-05,
      "loss": 2.8546,
      "step": 330
    },
    {
      "epoch": 0.42807680201447906,
      "grad_norm": 1.6725448369979858,
      "learning_rate": 4.288413098236776e-05,
      "loss": 2.8858,
      "step": 340
    },
    {
      "epoch": 0.4406672961913755,
      "grad_norm": 1.6339020729064941,
      "learning_rate": 4.2674223341729644e-05,
      "loss": 2.828,
      "step": 350
    },
    {
      "epoch": 0.45325779036827196,
      "grad_norm": 1.6598418951034546,
      "learning_rate": 4.246431570109152e-05,
      "loss": 2.8356,
      "step": 360
    },
    {
      "epoch": 0.4658482845451684,
      "grad_norm": 2.101541519165039,
      "learning_rate": 4.225440806045341e-05,
      "loss": 2.845,
      "step": 370
    },
    {
      "epoch": 0.47843877872206486,
      "grad_norm": 1.744951844215393,
      "learning_rate": 4.2044500419815286e-05,
      "loss": 2.7379,
      "step": 380
    },
    {
      "epoch": 0.4910292728989613,
      "grad_norm": 1.6617220640182495,
      "learning_rate": 4.1834592779177164e-05,
      "loss": 2.7887,
      "step": 390
    },
    {
      "epoch": 0.5036197670758578,
      "grad_norm": 1.6070425510406494,
      "learning_rate": 4.162468513853904e-05,
      "loss": 2.7866,
      "step": 400
    },
    {
      "epoch": 0.5162102612527542,
      "grad_norm": 1.7474422454833984,
      "learning_rate": 4.141477749790093e-05,
      "loss": 2.7525,
      "step": 410
    },
    {
      "epoch": 0.5288007554296507,
      "grad_norm": 1.8946878910064697,
      "learning_rate": 4.1204869857262806e-05,
      "loss": 2.8161,
      "step": 420
    },
    {
      "epoch": 0.541391249606547,
      "grad_norm": 1.6571087837219238,
      "learning_rate": 4.099496221662469e-05,
      "loss": 2.7995,
      "step": 430
    },
    {
      "epoch": 0.5539817437834434,
      "grad_norm": 1.7479621171951294,
      "learning_rate": 4.078505457598657e-05,
      "loss": 2.7922,
      "step": 440
    },
    {
      "epoch": 0.56657223796034,
      "grad_norm": 1.6450055837631226,
      "learning_rate": 4.057514693534845e-05,
      "loss": 2.804,
      "step": 450
    },
    {
      "epoch": 0.5791627321372363,
      "grad_norm": 1.9082242250442505,
      "learning_rate": 4.0365239294710326e-05,
      "loss": 2.7149,
      "step": 460
    },
    {
      "epoch": 0.5917532263141329,
      "grad_norm": 1.8304933309555054,
      "learning_rate": 4.015533165407221e-05,
      "loss": 2.7438,
      "step": 470
    },
    {
      "epoch": 0.6043437204910292,
      "grad_norm": 1.8375259637832642,
      "learning_rate": 3.994542401343409e-05,
      "loss": 2.818,
      "step": 480
    },
    {
      "epoch": 0.6169342146679258,
      "grad_norm": 1.925902247428894,
      "learning_rate": 3.9735516372795974e-05,
      "loss": 2.7682,
      "step": 490
    },
    {
      "epoch": 0.6295247088448221,
      "grad_norm": 1.8346632719039917,
      "learning_rate": 3.952560873215785e-05,
      "loss": 2.7995,
      "step": 500
    },
    {
      "epoch": 0.6421152030217187,
      "grad_norm": 1.7226344347000122,
      "learning_rate": 3.931570109151974e-05,
      "loss": 2.8302,
      "step": 510
    },
    {
      "epoch": 0.654705697198615,
      "grad_norm": 1.9931342601776123,
      "learning_rate": 3.910579345088161e-05,
      "loss": 2.7406,
      "step": 520
    },
    {
      "epoch": 0.6672961913755114,
      "grad_norm": 1.809956669807434,
      "learning_rate": 3.8895885810243495e-05,
      "loss": 2.7592,
      "step": 530
    },
    {
      "epoch": 0.6798866855524079,
      "grad_norm": 1.9489461183547974,
      "learning_rate": 3.868597816960537e-05,
      "loss": 2.7471,
      "step": 540
    },
    {
      "epoch": 0.6924771797293043,
      "grad_norm": 1.776718258857727,
      "learning_rate": 3.847607052896726e-05,
      "loss": 2.6973,
      "step": 550
    },
    {
      "epoch": 0.7050676739062008,
      "grad_norm": 1.6904932260513306,
      "learning_rate": 3.8266162888329136e-05,
      "loss": 2.7008,
      "step": 560
    },
    {
      "epoch": 0.7176581680830972,
      "grad_norm": 1.9774354696273804,
      "learning_rate": 3.805625524769102e-05,
      "loss": 2.7849,
      "step": 570
    },
    {
      "epoch": 0.7302486622599937,
      "grad_norm": 1.9746869802474976,
      "learning_rate": 3.78463476070529e-05,
      "loss": 2.7931,
      "step": 580
    },
    {
      "epoch": 0.7428391564368901,
      "grad_norm": 1.9054274559020996,
      "learning_rate": 3.763643996641478e-05,
      "loss": 2.7961,
      "step": 590
    },
    {
      "epoch": 0.7554296506137866,
      "grad_norm": 2.1122324466705322,
      "learning_rate": 3.7426532325776657e-05,
      "loss": 2.7354,
      "step": 600
    },
    {
      "epoch": 0.768020144790683,
      "grad_norm": 1.857917070388794,
      "learning_rate": 3.721662468513854e-05,
      "loss": 2.6802,
      "step": 610
    },
    {
      "epoch": 0.7806106389675794,
      "grad_norm": 1.7954232692718506,
      "learning_rate": 3.700671704450042e-05,
      "loss": 2.7936,
      "step": 620
    },
    {
      "epoch": 0.7932011331444759,
      "grad_norm": 2.000459671020508,
      "learning_rate": 3.6796809403862305e-05,
      "loss": 2.7137,
      "step": 630
    },
    {
      "epoch": 0.8057916273213723,
      "grad_norm": 2.0221219062805176,
      "learning_rate": 3.6586901763224183e-05,
      "loss": 2.6906,
      "step": 640
    },
    {
      "epoch": 0.8183821214982688,
      "grad_norm": 1.9394559860229492,
      "learning_rate": 3.637699412258607e-05,
      "loss": 2.7086,
      "step": 650
    },
    {
      "epoch": 0.8309726156751652,
      "grad_norm": 2.0686452388763428,
      "learning_rate": 3.616708648194794e-05,
      "loss": 2.6908,
      "step": 660
    },
    {
      "epoch": 0.8435631098520617,
      "grad_norm": 2.0316474437713623,
      "learning_rate": 3.5957178841309825e-05,
      "loss": 2.7607,
      "step": 670
    },
    {
      "epoch": 0.8561536040289581,
      "grad_norm": 2.029508590698242,
      "learning_rate": 3.5747271200671704e-05,
      "loss": 2.6634,
      "step": 680
    },
    {
      "epoch": 0.8687440982058546,
      "grad_norm": 2.0051140785217285,
      "learning_rate": 3.553736356003359e-05,
      "loss": 2.7237,
      "step": 690
    },
    {
      "epoch": 0.881334592382751,
      "grad_norm": 1.8838136196136475,
      "learning_rate": 3.532745591939547e-05,
      "loss": 2.7023,
      "step": 700
    },
    {
      "epoch": 0.8939250865596474,
      "grad_norm": 2.0159034729003906,
      "learning_rate": 3.511754827875735e-05,
      "loss": 2.7557,
      "step": 710
    },
    {
      "epoch": 0.9065155807365439,
      "grad_norm": 1.8414585590362549,
      "learning_rate": 3.4907640638119224e-05,
      "loss": 2.6839,
      "step": 720
    },
    {
      "epoch": 0.9191060749134403,
      "grad_norm": 1.9694514274597168,
      "learning_rate": 3.469773299748111e-05,
      "loss": 2.7255,
      "step": 730
    },
    {
      "epoch": 0.9316965690903368,
      "grad_norm": 2.0455684661865234,
      "learning_rate": 3.448782535684299e-05,
      "loss": 2.7117,
      "step": 740
    },
    {
      "epoch": 0.9442870632672332,
      "grad_norm": 2.1715493202209473,
      "learning_rate": 3.427791771620487e-05,
      "loss": 2.6958,
      "step": 750
    },
    {
      "epoch": 0.9568775574441297,
      "grad_norm": 2.101524829864502,
      "learning_rate": 3.406801007556675e-05,
      "loss": 2.7591,
      "step": 760
    },
    {
      "epoch": 0.9694680516210261,
      "grad_norm": 1.981054425239563,
      "learning_rate": 3.3858102434928636e-05,
      "loss": 2.6403,
      "step": 770
    },
    {
      "epoch": 0.9820585457979226,
      "grad_norm": 1.9542646408081055,
      "learning_rate": 3.3648194794290514e-05,
      "loss": 2.803,
      "step": 780
    },
    {
      "epoch": 0.994649039974819,
      "grad_norm": 1.8838430643081665,
      "learning_rate": 3.343828715365239e-05,
      "loss": 2.686,
      "step": 790
    },
    {
      "epoch": 1.0062952470884483,
      "grad_norm": 2.1076722145080566,
      "learning_rate": 3.322837951301427e-05,
      "loss": 2.6158,
      "step": 800
    },
    {
      "epoch": 1.0188857412653447,
      "grad_norm": 2.05629301071167,
      "learning_rate": 3.3018471872376156e-05,
      "loss": 2.6827,
      "step": 810
    },
    {
      "epoch": 1.031476235442241,
      "grad_norm": 1.9842877388000488,
      "learning_rate": 3.2808564231738034e-05,
      "loss": 2.6458,
      "step": 820
    },
    {
      "epoch": 1.0440667296191375,
      "grad_norm": 2.197007179260254,
      "learning_rate": 3.259865659109992e-05,
      "loss": 2.6641,
      "step": 830
    },
    {
      "epoch": 1.056657223796034,
      "grad_norm": 2.307187557220459,
      "learning_rate": 3.23887489504618e-05,
      "loss": 2.6567,
      "step": 840
    },
    {
      "epoch": 1.0692477179729305,
      "grad_norm": 2.3088724613189697,
      "learning_rate": 3.217884130982368e-05,
      "loss": 2.6432,
      "step": 850
    },
    {
      "epoch": 1.081838212149827,
      "grad_norm": 2.4893996715545654,
      "learning_rate": 3.196893366918556e-05,
      "loss": 2.6388,
      "step": 860
    },
    {
      "epoch": 1.0944287063267233,
      "grad_norm": 2.1227786540985107,
      "learning_rate": 3.175902602854744e-05,
      "loss": 2.6975,
      "step": 870
    },
    {
      "epoch": 1.1070192005036197,
      "grad_norm": 2.0876076221466064,
      "learning_rate": 3.154911838790932e-05,
      "loss": 2.6876,
      "step": 880
    },
    {
      "epoch": 1.1196096946805163,
      "grad_norm": 2.1140801906585693,
      "learning_rate": 3.13392107472712e-05,
      "loss": 2.6749,
      "step": 890
    },
    {
      "epoch": 1.1322001888574127,
      "grad_norm": 2.566739082336426,
      "learning_rate": 3.112930310663308e-05,
      "loss": 2.7244,
      "step": 900
    },
    {
      "epoch": 1.144790683034309,
      "grad_norm": 2.3445916175842285,
      "learning_rate": 3.0919395465994966e-05,
      "loss": 2.6618,
      "step": 910
    },
    {
      "epoch": 1.1573811772112055,
      "grad_norm": 2.3953161239624023,
      "learning_rate": 3.0709487825356845e-05,
      "loss": 2.7212,
      "step": 920
    },
    {
      "epoch": 1.1699716713881019,
      "grad_norm": 2.0936362743377686,
      "learning_rate": 3.0499580184718723e-05,
      "loss": 2.6202,
      "step": 930
    },
    {
      "epoch": 1.1825621655649985,
      "grad_norm": 2.39625883102417,
      "learning_rate": 3.0289672544080605e-05,
      "loss": 2.6518,
      "step": 940
    },
    {
      "epoch": 1.1951526597418949,
      "grad_norm": 2.1260733604431152,
      "learning_rate": 3.0079764903442486e-05,
      "loss": 2.6744,
      "step": 950
    },
    {
      "epoch": 1.2077431539187913,
      "grad_norm": 2.0598418712615967,
      "learning_rate": 2.9869857262804368e-05,
      "loss": 2.7104,
      "step": 960
    },
    {
      "epoch": 1.2203336480956877,
      "grad_norm": 2.0865540504455566,
      "learning_rate": 2.965994962216625e-05,
      "loss": 2.62,
      "step": 970
    },
    {
      "epoch": 1.2329241422725843,
      "grad_norm": 2.2803704738616943,
      "learning_rate": 2.945004198152813e-05,
      "loss": 2.6693,
      "step": 980
    },
    {
      "epoch": 1.2455146364494807,
      "grad_norm": 2.456705093383789,
      "learning_rate": 2.9240134340890007e-05,
      "loss": 2.6815,
      "step": 990
    },
    {
      "epoch": 1.258105130626377,
      "grad_norm": 2.4568581581115723,
      "learning_rate": 2.903022670025189e-05,
      "loss": 2.7265,
      "step": 1000
    },
    {
      "epoch": 1.2706956248032735,
      "grad_norm": 2.3564963340759277,
      "learning_rate": 2.882031905961377e-05,
      "loss": 2.7145,
      "step": 1010
    },
    {
      "epoch": 1.28328611898017,
      "grad_norm": 2.295034170150757,
      "learning_rate": 2.8610411418975652e-05,
      "loss": 2.6914,
      "step": 1020
    },
    {
      "epoch": 1.2958766131570665,
      "grad_norm": 2.138969659805298,
      "learning_rate": 2.8400503778337533e-05,
      "loss": 2.6784,
      "step": 1030
    },
    {
      "epoch": 1.3084671073339629,
      "grad_norm": 2.482337236404419,
      "learning_rate": 2.8190596137699415e-05,
      "loss": 2.586,
      "step": 1040
    },
    {
      "epoch": 1.3210576015108593,
      "grad_norm": 2.394423007965088,
      "learning_rate": 2.7980688497061297e-05,
      "loss": 2.7148,
      "step": 1050
    },
    {
      "epoch": 1.3336480956877557,
      "grad_norm": 2.46347975730896,
      "learning_rate": 2.7770780856423172e-05,
      "loss": 2.6413,
      "step": 1060
    },
    {
      "epoch": 1.3462385898646523,
      "grad_norm": 2.1130530834198,
      "learning_rate": 2.7560873215785054e-05,
      "loss": 2.6398,
      "step": 1070
    },
    {
      "epoch": 1.3588290840415487,
      "grad_norm": 2.118363380432129,
      "learning_rate": 2.7350965575146935e-05,
      "loss": 2.6362,
      "step": 1080
    },
    {
      "epoch": 1.371419578218445,
      "grad_norm": 2.5593628883361816,
      "learning_rate": 2.7141057934508817e-05,
      "loss": 2.6312,
      "step": 1090
    },
    {
      "epoch": 1.3840100723953415,
      "grad_norm": 2.230656385421753,
      "learning_rate": 2.69311502938707e-05,
      "loss": 2.6639,
      "step": 1100
    },
    {
      "epoch": 1.3966005665722379,
      "grad_norm": 2.4319565296173096,
      "learning_rate": 2.672124265323258e-05,
      "loss": 2.5975,
      "step": 1110
    },
    {
      "epoch": 1.4091910607491345,
      "grad_norm": 2.3552396297454834,
      "learning_rate": 2.6511335012594455e-05,
      "loss": 2.6765,
      "step": 1120
    },
    {
      "epoch": 1.4217815549260309,
      "grad_norm": 2.4763829708099365,
      "learning_rate": 2.6301427371956337e-05,
      "loss": 2.7301,
      "step": 1130
    },
    {
      "epoch": 1.4343720491029273,
      "grad_norm": 2.562966823577881,
      "learning_rate": 2.609151973131822e-05,
      "loss": 2.7065,
      "step": 1140
    },
    {
      "epoch": 1.4469625432798237,
      "grad_norm": 2.4291951656341553,
      "learning_rate": 2.58816120906801e-05,
      "loss": 2.7009,
      "step": 1150
    },
    {
      "epoch": 1.45955303745672,
      "grad_norm": 2.3923776149749756,
      "learning_rate": 2.5671704450041982e-05,
      "loss": 2.69,
      "step": 1160
    },
    {
      "epoch": 1.4721435316336167,
      "grad_norm": 2.504021167755127,
      "learning_rate": 2.5461796809403864e-05,
      "loss": 2.7126,
      "step": 1170
    },
    {
      "epoch": 1.484734025810513,
      "grad_norm": 2.3801121711730957,
      "learning_rate": 2.5251889168765746e-05,
      "loss": 2.6741,
      "step": 1180
    },
    {
      "epoch": 1.4973245199874095,
      "grad_norm": 2.3505077362060547,
      "learning_rate": 2.5041981528127624e-05,
      "loss": 2.6675,
      "step": 1190
    },
    {
      "epoch": 1.509915014164306,
      "grad_norm": 2.695152997970581,
      "learning_rate": 2.4832073887489506e-05,
      "loss": 2.7081,
      "step": 1200
    },
    {
      "epoch": 1.5225055083412022,
      "grad_norm": 2.259963035583496,
      "learning_rate": 2.4622166246851384e-05,
      "loss": 2.6533,
      "step": 1210
    },
    {
      "epoch": 1.5350960025180989,
      "grad_norm": 2.3512771129608154,
      "learning_rate": 2.4412258606213266e-05,
      "loss": 2.6158,
      "step": 1220
    },
    {
      "epoch": 1.5476864966949953,
      "grad_norm": 2.557711124420166,
      "learning_rate": 2.4202350965575148e-05,
      "loss": 2.6027,
      "step": 1230
    },
    {
      "epoch": 1.5602769908718916,
      "grad_norm": 2.8509936332702637,
      "learning_rate": 2.399244332493703e-05,
      "loss": 2.7013,
      "step": 1240
    },
    {
      "epoch": 1.5728674850487883,
      "grad_norm": 3.1254360675811768,
      "learning_rate": 2.3782535684298908e-05,
      "loss": 2.6178,
      "step": 1250
    },
    {
      "epoch": 1.5854579792256847,
      "grad_norm": 2.8501224517822266,
      "learning_rate": 2.357262804366079e-05,
      "loss": 2.5906,
      "step": 1260
    },
    {
      "epoch": 1.598048473402581,
      "grad_norm": 2.5215115547180176,
      "learning_rate": 2.336272040302267e-05,
      "loss": 2.6524,
      "step": 1270
    },
    {
      "epoch": 1.6106389675794774,
      "grad_norm": 2.5775210857391357,
      "learning_rate": 2.3152812762384553e-05,
      "loss": 2.616,
      "step": 1280
    },
    {
      "epoch": 1.6232294617563738,
      "grad_norm": 2.413086414337158,
      "learning_rate": 2.2942905121746435e-05,
      "loss": 2.6496,
      "step": 1290
    },
    {
      "epoch": 1.6358199559332705,
      "grad_norm": 2.7004477977752686,
      "learning_rate": 2.2732997481108313e-05,
      "loss": 2.658,
      "step": 1300
    },
    {
      "epoch": 1.6484104501101668,
      "grad_norm": 2.215217351913452,
      "learning_rate": 2.2523089840470195e-05,
      "loss": 2.6427,
      "step": 1310
    },
    {
      "epoch": 1.6610009442870632,
      "grad_norm": 2.810631036758423,
      "learning_rate": 2.2313182199832076e-05,
      "loss": 2.7006,
      "step": 1320
    },
    {
      "epoch": 1.6735914384639599,
      "grad_norm": 2.446225643157959,
      "learning_rate": 2.2103274559193958e-05,
      "loss": 2.6489,
      "step": 1330
    },
    {
      "epoch": 1.686181932640856,
      "grad_norm": 2.49072527885437,
      "learning_rate": 2.1893366918555837e-05,
      "loss": 2.6256,
      "step": 1340
    },
    {
      "epoch": 1.6987724268177526,
      "grad_norm": 2.3313286304473877,
      "learning_rate": 2.1683459277917718e-05,
      "loss": 2.6313,
      "step": 1350
    },
    {
      "epoch": 1.711362920994649,
      "grad_norm": 2.615863800048828,
      "learning_rate": 2.14735516372796e-05,
      "loss": 2.6123,
      "step": 1360
    },
    {
      "epoch": 1.7239534151715454,
      "grad_norm": 2.7533864974975586,
      "learning_rate": 2.126364399664148e-05,
      "loss": 2.6887,
      "step": 1370
    },
    {
      "epoch": 1.736543909348442,
      "grad_norm": 2.412465810775757,
      "learning_rate": 2.105373635600336e-05,
      "loss": 2.6635,
      "step": 1380
    },
    {
      "epoch": 1.7491344035253382,
      "grad_norm": 2.708475351333618,
      "learning_rate": 2.0843828715365242e-05,
      "loss": 2.6599,
      "step": 1390
    },
    {
      "epoch": 1.7617248977022348,
      "grad_norm": 2.5110154151916504,
      "learning_rate": 2.0633921074727123e-05,
      "loss": 2.6381,
      "step": 1400
    },
    {
      "epoch": 1.7743153918791312,
      "grad_norm": 2.5354177951812744,
      "learning_rate": 2.0424013434089002e-05,
      "loss": 2.6838,
      "step": 1410
    },
    {
      "epoch": 1.7869058860560276,
      "grad_norm": 2.4484012126922607,
      "learning_rate": 2.0214105793450884e-05,
      "loss": 2.6778,
      "step": 1420
    },
    {
      "epoch": 1.7994963802329242,
      "grad_norm": 2.679189920425415,
      "learning_rate": 2.0004198152812765e-05,
      "loss": 2.6994,
      "step": 1430
    },
    {
      "epoch": 1.8120868744098206,
      "grad_norm": 2.435702085494995,
      "learning_rate": 1.9794290512174644e-05,
      "loss": 2.5828,
      "step": 1440
    },
    {
      "epoch": 1.824677368586717,
      "grad_norm": 2.5143513679504395,
      "learning_rate": 1.9584382871536525e-05,
      "loss": 2.6171,
      "step": 1450
    },
    {
      "epoch": 1.8372678627636134,
      "grad_norm": 2.8438022136688232,
      "learning_rate": 1.9374475230898407e-05,
      "loss": 2.6342,
      "step": 1460
    },
    {
      "epoch": 1.8498583569405098,
      "grad_norm": 2.5723841190338135,
      "learning_rate": 1.9164567590260285e-05,
      "loss": 2.6165,
      "step": 1470
    },
    {
      "epoch": 1.8624488511174064,
      "grad_norm": 2.7525746822357178,
      "learning_rate": 1.8954659949622167e-05,
      "loss": 2.5716,
      "step": 1480
    },
    {
      "epoch": 1.8750393452943028,
      "grad_norm": 2.899754524230957,
      "learning_rate": 1.874475230898405e-05,
      "loss": 2.6628,
      "step": 1490
    },
    {
      "epoch": 1.8876298394711992,
      "grad_norm": 2.4716973304748535,
      "learning_rate": 1.853484466834593e-05,
      "loss": 2.6633,
      "step": 1500
    },
    {
      "epoch": 1.9002203336480958,
      "grad_norm": 2.453744649887085,
      "learning_rate": 1.832493702770781e-05,
      "loss": 2.689,
      "step": 1510
    },
    {
      "epoch": 1.912810827824992,
      "grad_norm": 2.8467655181884766,
      "learning_rate": 1.811502938706969e-05,
      "loss": 2.6447,
      "step": 1520
    },
    {
      "epoch": 1.9254013220018886,
      "grad_norm": 2.7346031665802,
      "learning_rate": 1.7905121746431572e-05,
      "loss": 2.6176,
      "step": 1530
    },
    {
      "epoch": 1.937991816178785,
      "grad_norm": 2.590641498565674,
      "learning_rate": 1.769521410579345e-05,
      "loss": 2.5418,
      "step": 1540
    },
    {
      "epoch": 1.9505823103556814,
      "grad_norm": 2.817537784576416,
      "learning_rate": 1.7485306465155332e-05,
      "loss": 2.5968,
      "step": 1550
    },
    {
      "epoch": 1.963172804532578,
      "grad_norm": 2.7719595432281494,
      "learning_rate": 1.7275398824517214e-05,
      "loss": 2.6575,
      "step": 1560
    },
    {
      "epoch": 1.9757632987094742,
      "grad_norm": 2.7548866271972656,
      "learning_rate": 1.7065491183879096e-05,
      "loss": 2.6072,
      "step": 1570
    },
    {
      "epoch": 1.9883537928863708,
      "grad_norm": 2.652911901473999,
      "learning_rate": 1.6855583543240974e-05,
      "loss": 2.6892,
      "step": 1580
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.660031318664551,
      "learning_rate": 1.6645675902602856e-05,
      "loss": 2.6758,
      "step": 1590
    },
    {
      "epoch": 2.0125904941768966,
      "grad_norm": 2.7798659801483154,
      "learning_rate": 1.6435768261964738e-05,
      "loss": 2.679,
      "step": 1600
    },
    {
      "epoch": 2.025180988353793,
      "grad_norm": 2.8389179706573486,
      "learning_rate": 1.6225860621326616e-05,
      "loss": 2.5981,
      "step": 1610
    },
    {
      "epoch": 2.0377714825306894,
      "grad_norm": 2.6866953372955322,
      "learning_rate": 1.6015952980688498e-05,
      "loss": 2.5933,
      "step": 1620
    },
    {
      "epoch": 2.0503619767075856,
      "grad_norm": 3.147181510925293,
      "learning_rate": 1.580604534005038e-05,
      "loss": 2.5741,
      "step": 1630
    },
    {
      "epoch": 2.062952470884482,
      "grad_norm": 2.7777152061462402,
      "learning_rate": 1.5596137699412258e-05,
      "loss": 2.6524,
      "step": 1640
    },
    {
      "epoch": 2.075542965061379,
      "grad_norm": 2.4648780822753906,
      "learning_rate": 1.538623005877414e-05,
      "loss": 2.5991,
      "step": 1650
    },
    {
      "epoch": 2.088133459238275,
      "grad_norm": 2.9806041717529297,
      "learning_rate": 1.5176322418136021e-05,
      "loss": 2.5725,
      "step": 1660
    },
    {
      "epoch": 2.1007239534151716,
      "grad_norm": 2.686349391937256,
      "learning_rate": 1.4966414777497903e-05,
      "loss": 2.6932,
      "step": 1670
    },
    {
      "epoch": 2.113314447592068,
      "grad_norm": 2.61130428314209,
      "learning_rate": 1.4756507136859781e-05,
      "loss": 2.6475,
      "step": 1680
    },
    {
      "epoch": 2.1259049417689644,
      "grad_norm": 2.9027585983276367,
      "learning_rate": 1.4546599496221663e-05,
      "loss": 2.5603,
      "step": 1690
    },
    {
      "epoch": 2.138495435945861,
      "grad_norm": 3.090635061264038,
      "learning_rate": 1.4336691855583545e-05,
      "loss": 2.6279,
      "step": 1700
    },
    {
      "epoch": 2.151085930122757,
      "grad_norm": 2.93871808052063,
      "learning_rate": 1.4126784214945423e-05,
      "loss": 2.5553,
      "step": 1710
    },
    {
      "epoch": 2.163676424299654,
      "grad_norm": 2.822366714477539,
      "learning_rate": 1.3916876574307305e-05,
      "loss": 2.4974,
      "step": 1720
    },
    {
      "epoch": 2.1762669184765504,
      "grad_norm": 3.452942371368408,
      "learning_rate": 1.3706968933669187e-05,
      "loss": 2.6135,
      "step": 1730
    },
    {
      "epoch": 2.1888574126534466,
      "grad_norm": 2.620570659637451,
      "learning_rate": 1.3497061293031067e-05,
      "loss": 2.6234,
      "step": 1740
    },
    {
      "epoch": 2.201447906830343,
      "grad_norm": 3.168222188949585,
      "learning_rate": 1.3287153652392947e-05,
      "loss": 2.5172,
      "step": 1750
    },
    {
      "epoch": 2.2140384010072394,
      "grad_norm": 2.5447795391082764,
      "learning_rate": 1.3077246011754828e-05,
      "loss": 2.6293,
      "step": 1760
    },
    {
      "epoch": 2.226628895184136,
      "grad_norm": 2.8931479454040527,
      "learning_rate": 1.286733837111671e-05,
      "loss": 2.5639,
      "step": 1770
    },
    {
      "epoch": 2.2392193893610326,
      "grad_norm": 2.6708476543426514,
      "learning_rate": 1.265743073047859e-05,
      "loss": 2.6651,
      "step": 1780
    },
    {
      "epoch": 2.2518098835379288,
      "grad_norm": 2.790078639984131,
      "learning_rate": 1.244752308984047e-05,
      "loss": 2.572,
      "step": 1790
    },
    {
      "epoch": 2.2644003777148254,
      "grad_norm": 2.678032636642456,
      "learning_rate": 1.2237615449202352e-05,
      "loss": 2.5925,
      "step": 1800
    },
    {
      "epoch": 2.2769908718917216,
      "grad_norm": 2.853641986846924,
      "learning_rate": 1.2027707808564234e-05,
      "loss": 2.6283,
      "step": 1810
    },
    {
      "epoch": 2.289581366068618,
      "grad_norm": 3.030682325363159,
      "learning_rate": 1.1817800167926114e-05,
      "loss": 2.6267,
      "step": 1820
    },
    {
      "epoch": 2.302171860245515,
      "grad_norm": 2.758025646209717,
      "learning_rate": 1.1607892527287994e-05,
      "loss": 2.6813,
      "step": 1830
    },
    {
      "epoch": 2.314762354422411,
      "grad_norm": 2.8158938884735107,
      "learning_rate": 1.1397984886649875e-05,
      "loss": 2.5389,
      "step": 1840
    },
    {
      "epoch": 2.3273528485993076,
      "grad_norm": 2.883148193359375,
      "learning_rate": 1.1188077246011755e-05,
      "loss": 2.5544,
      "step": 1850
    },
    {
      "epoch": 2.3399433427762037,
      "grad_norm": 3.2053632736206055,
      "learning_rate": 1.0978169605373637e-05,
      "loss": 2.575,
      "step": 1860
    },
    {
      "epoch": 2.3525338369531004,
      "grad_norm": 3.021407127380371,
      "learning_rate": 1.0768261964735517e-05,
      "loss": 2.5936,
      "step": 1870
    },
    {
      "epoch": 2.365124331129997,
      "grad_norm": 2.7500908374786377,
      "learning_rate": 1.0558354324097397e-05,
      "loss": 2.524,
      "step": 1880
    },
    {
      "epoch": 2.377714825306893,
      "grad_norm": 2.858488082885742,
      "learning_rate": 1.0348446683459279e-05,
      "loss": 2.6806,
      "step": 1890
    },
    {
      "epoch": 2.3903053194837898,
      "grad_norm": 2.971672296524048,
      "learning_rate": 1.0138539042821159e-05,
      "loss": 2.5809,
      "step": 1900
    },
    {
      "epoch": 2.4028958136606864,
      "grad_norm": 2.792813777923584,
      "learning_rate": 9.92863140218304e-06,
      "loss": 2.5635,
      "step": 1910
    },
    {
      "epoch": 2.4154863078375826,
      "grad_norm": 3.181762933731079,
      "learning_rate": 9.71872376154492e-06,
      "loss": 2.6313,
      "step": 1920
    },
    {
      "epoch": 2.428076802014479,
      "grad_norm": 2.950153112411499,
      "learning_rate": 9.5088161209068e-06,
      "loss": 2.5975,
      "step": 1930
    },
    {
      "epoch": 2.4406672961913753,
      "grad_norm": 3.0553805828094482,
      "learning_rate": 9.298908480268683e-06,
      "loss": 2.5737,
      "step": 1940
    },
    {
      "epoch": 2.453257790368272,
      "grad_norm": 2.768237352371216,
      "learning_rate": 9.089000839630563e-06,
      "loss": 2.6265,
      "step": 1950
    },
    {
      "epoch": 2.4658482845451686,
      "grad_norm": 2.7396597862243652,
      "learning_rate": 8.879093198992444e-06,
      "loss": 2.573,
      "step": 1960
    },
    {
      "epoch": 2.4784387787220648,
      "grad_norm": 3.6874353885650635,
      "learning_rate": 8.669185558354324e-06,
      "loss": 2.6218,
      "step": 1970
    },
    {
      "epoch": 2.4910292728989614,
      "grad_norm": 2.928029775619507,
      "learning_rate": 8.459277917716204e-06,
      "loss": 2.5835,
      "step": 1980
    },
    {
      "epoch": 2.5036197670758575,
      "grad_norm": 2.7691633701324463,
      "learning_rate": 8.249370277078086e-06,
      "loss": 2.6899,
      "step": 1990
    },
    {
      "epoch": 2.516210261252754,
      "grad_norm": 3.266862392425537,
      "learning_rate": 8.039462636439966e-06,
      "loss": 2.5993,
      "step": 2000
    },
    {
      "epoch": 2.5288007554296508,
      "grad_norm": 3.0033864974975586,
      "learning_rate": 7.829554995801848e-06,
      "loss": 2.5761,
      "step": 2010
    },
    {
      "epoch": 2.541391249606547,
      "grad_norm": 3.162954568862915,
      "learning_rate": 7.619647355163728e-06,
      "loss": 2.6287,
      "step": 2020
    },
    {
      "epoch": 2.5539817437834436,
      "grad_norm": 2.6276073455810547,
      "learning_rate": 7.40973971452561e-06,
      "loss": 2.5004,
      "step": 2030
    },
    {
      "epoch": 2.56657223796034,
      "grad_norm": 3.1272990703582764,
      "learning_rate": 7.19983207388749e-06,
      "loss": 2.5885,
      "step": 2040
    },
    {
      "epoch": 2.5791627321372363,
      "grad_norm": 3.1486403942108154,
      "learning_rate": 6.9899244332493705e-06,
      "loss": 2.617,
      "step": 2050
    },
    {
      "epoch": 2.591753226314133,
      "grad_norm": 2.8203158378601074,
      "learning_rate": 6.780016792611251e-06,
      "loss": 2.672,
      "step": 2060
    },
    {
      "epoch": 2.604343720491029,
      "grad_norm": 2.878647565841675,
      "learning_rate": 6.570109151973132e-06,
      "loss": 2.6066,
      "step": 2070
    },
    {
      "epoch": 2.6169342146679258,
      "grad_norm": 2.9596707820892334,
      "learning_rate": 6.360201511335013e-06,
      "loss": 2.5278,
      "step": 2080
    },
    {
      "epoch": 2.629524708844822,
      "grad_norm": 3.1903598308563232,
      "learning_rate": 6.150293870696894e-06,
      "loss": 2.5798,
      "step": 2090
    },
    {
      "epoch": 2.6421152030217185,
      "grad_norm": 3.008258104324341,
      "learning_rate": 5.940386230058775e-06,
      "loss": 2.5126,
      "step": 2100
    },
    {
      "epoch": 2.654705697198615,
      "grad_norm": 3.1267917156219482,
      "learning_rate": 5.730478589420655e-06,
      "loss": 2.5603,
      "step": 2110
    },
    {
      "epoch": 2.6672961913755113,
      "grad_norm": 3.0903854370117188,
      "learning_rate": 5.520570948782536e-06,
      "loss": 2.5888,
      "step": 2120
    },
    {
      "epoch": 2.679886685552408,
      "grad_norm": 2.9022789001464844,
      "learning_rate": 5.310663308144417e-06,
      "loss": 2.5573,
      "step": 2130
    },
    {
      "epoch": 2.6924771797293046,
      "grad_norm": 3.199079751968384,
      "learning_rate": 5.100755667506298e-06,
      "loss": 2.5729,
      "step": 2140
    },
    {
      "epoch": 2.7050676739062007,
      "grad_norm": 3.1090927124023438,
      "learning_rate": 4.8908480268681785e-06,
      "loss": 2.5966,
      "step": 2150
    },
    {
      "epoch": 2.7176581680830973,
      "grad_norm": 3.049743890762329,
      "learning_rate": 4.6809403862300585e-06,
      "loss": 2.5593,
      "step": 2160
    },
    {
      "epoch": 2.730248662259994,
      "grad_norm": 3.2856998443603516,
      "learning_rate": 4.471032745591939e-06,
      "loss": 2.5559,
      "step": 2170
    },
    {
      "epoch": 2.74283915643689,
      "grad_norm": 2.9537131786346436,
      "learning_rate": 4.26112510495382e-06,
      "loss": 2.5735,
      "step": 2180
    },
    {
      "epoch": 2.7554296506137868,
      "grad_norm": 3.0146424770355225,
      "learning_rate": 4.051217464315701e-06,
      "loss": 2.5644,
      "step": 2190
    },
    {
      "epoch": 2.768020144790683,
      "grad_norm": 2.920738697052002,
      "learning_rate": 3.841309823677582e-06,
      "loss": 2.6272,
      "step": 2200
    },
    {
      "epoch": 2.7806106389675795,
      "grad_norm": 3.0443835258483887,
      "learning_rate": 3.6314021830394625e-06,
      "loss": 2.6373,
      "step": 2210
    },
    {
      "epoch": 2.7932011331444757,
      "grad_norm": 3.1115989685058594,
      "learning_rate": 3.4214945424013434e-06,
      "loss": 2.5334,
      "step": 2220
    },
    {
      "epoch": 2.8057916273213723,
      "grad_norm": 2.943390130996704,
      "learning_rate": 3.2115869017632242e-06,
      "loss": 2.6069,
      "step": 2230
    },
    {
      "epoch": 2.818382121498269,
      "grad_norm": 2.7366831302642822,
      "learning_rate": 3.001679261125105e-06,
      "loss": 2.6049,
      "step": 2240
    },
    {
      "epoch": 2.830972615675165,
      "grad_norm": 2.863823652267456,
      "learning_rate": 2.7917716204869856e-06,
      "loss": 2.6248,
      "step": 2250
    },
    {
      "epoch": 2.8435631098520617,
      "grad_norm": 3.054800271987915,
      "learning_rate": 2.5818639798488664e-06,
      "loss": 2.6727,
      "step": 2260
    },
    {
      "epoch": 2.8561536040289583,
      "grad_norm": 3.2210752964019775,
      "learning_rate": 2.3719563392107473e-06,
      "loss": 2.6266,
      "step": 2270
    },
    {
      "epoch": 2.8687440982058545,
      "grad_norm": 3.20285701751709,
      "learning_rate": 2.162048698572628e-06,
      "loss": 2.5771,
      "step": 2280
    },
    {
      "epoch": 2.881334592382751,
      "grad_norm": 2.835261583328247,
      "learning_rate": 1.952141057934509e-06,
      "loss": 2.5284,
      "step": 2290
    },
    {
      "epoch": 2.8939250865596473,
      "grad_norm": 2.9562811851501465,
      "learning_rate": 1.7422334172963895e-06,
      "loss": 2.5516,
      "step": 2300
    },
    {
      "epoch": 2.906515580736544,
      "grad_norm": 3.1310269832611084,
      "learning_rate": 1.5323257766582704e-06,
      "loss": 2.6177,
      "step": 2310
    },
    {
      "epoch": 2.91910607491344,
      "grad_norm": 3.2334353923797607,
      "learning_rate": 1.322418136020151e-06,
      "loss": 2.6091,
      "step": 2320
    },
    {
      "epoch": 2.9316965690903367,
      "grad_norm": 2.7932803630828857,
      "learning_rate": 1.112510495382032e-06,
      "loss": 2.5656,
      "step": 2330
    },
    {
      "epoch": 2.9442870632672333,
      "grad_norm": 2.841499090194702,
      "learning_rate": 9.026028547439127e-07,
      "loss": 2.5609,
      "step": 2340
    },
    {
      "epoch": 2.9568775574441295,
      "grad_norm": 3.7554209232330322,
      "learning_rate": 6.926952141057935e-07,
      "loss": 2.6266,
      "step": 2350
    },
    {
      "epoch": 2.969468051621026,
      "grad_norm": 3.0381290912628174,
      "learning_rate": 4.827875734676742e-07,
      "loss": 2.5604,
      "step": 2360
    },
    {
      "epoch": 2.9820585457979227,
      "grad_norm": 3.230900526046753,
      "learning_rate": 2.72879932829555e-07,
      "loss": 2.5939,
      "step": 2370
    },
    {
      "epoch": 2.994649039974819,
      "grad_norm": 3.14922833442688,
      "learning_rate": 6.297229219143578e-08,
      "loss": 2.6225,
      "step": 2380
    }
  ],
  "logging_steps": 10,
  "max_steps": 2382,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.372658441762898e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
